{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpsuG/+KEVoqcsubx6doHD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DefinitelyKev/nlp_bert_ml_recipe_models/blob/main/whole_model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xoLVTxSzupRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[onnxruntime] onnx"
      ],
      "metadata": {
        "id": "H-T7MklI6gkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSmVoJczpeX_",
        "outputId": "f95b59de-ab4c-431d-d835-f9bec5ce9ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torch.nn.functional import softmax\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from transformers import DistilBertConfig\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Path**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_azfrWCLusZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ingredient_parsing_model_path = '/content/drive/MyDrive/ingredient_parsing_model'\n",
        "\n",
        "segmentation_model_path = '/content/drive/MyDrive/segmentation_model/segmentation_model_v1'\n",
        "segmentation_model_path_onnx = '/content/drive/MyDrive/segmentation_model/segmentation_model_v1.onnx'\n",
        "segmentation_le_path = '/content/drive/MyDrive/segmentation_model/segmentation_model_label_encoder_v1/le.pkl'\n",
        "\n",
        "cook_time_model_path = '/content/drive/MyDrive/cook_time_model/cook_time_model_v1'\n",
        "cook_time_model_path_onnx = '/content/drive/MyDrive/cook_time_model/cook_time_model_v1.onnx'\n",
        "cook_time_scaler_path = '/content/drive/MyDrive/cook_time_model/cook_time_scaler_v1/scaler.pkl'\n",
        "\n",
        "tags_model_path = '/content/drive/MyDrive/tagging_model_tags/tags_model_v1'\n",
        "tags_model_path_onnx = '/content/drive/MyDrive/tagging_model_tags/tags_model_v1.onnx'\n",
        "tags_mlb_path = '/content/drive/MyDrive/tagging_model_tags/tags_mlb_v1/mlb.pkl'\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", token=access_token)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "nSerdVrFucYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9a6bb3-b7eb-481e-d4a3-38a66bb99101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Definitions**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Y0aQQWTuwDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class AttentionLayer(nn.Module):\n",
        "#     def __init__(self, in_features):\n",
        "#         super(AttentionLayer, self).__init__()\n",
        "#         self.attention = nn.Sequential(\n",
        "#             nn.Linear(in_features, in_features),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(in_features, 1),\n",
        "#             nn.Softmax(dim=1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         weights = self.attention(x)\n",
        "#         weighted = torch.mul(x, weights.expand_as(x))\n",
        "#         return weighted.sum(1)"
      ],
      "metadata": {
        "id": "WC-ztFXFeiDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model Definition\n",
        "# class DistilBertRegressor(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(DistilBertRegressor, self).__init__()\n",
        "#         self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", token=access_token)\n",
        "#         self.config = self.distilbert.config\n",
        "#         self.attention = AttentionLayer(self.distilbert.config.dim)\n",
        "#         self.regressor = nn.Sequential(\n",
        "#             nn.Dropout(0.2),\n",
        "#             nn.Linear(self.distilbert.config.dim, 1024),  # Increased input layer size\n",
        "#             nn.LayerNorm(1024),\n",
        "#             nn.GELU(),\n",
        "#             nn.Dropout(0.1),\n",
        "#             nn.Linear(1024, 512),\n",
        "#             nn.LayerNorm(512),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(512, 256),\n",
        "#             nn.LayerNorm(256),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.LayerNorm(128),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(128, 1)  # Output layer for two targets (cook_time, prep_time)\n",
        "#         )\n",
        "#         # Enhanced residual path with layer normalization\n",
        "#         self.residual = nn.Sequential(\n",
        "#             nn.Linear(self.distilbert.config.dim, 512),\n",
        "#             nn.LayerNorm(512),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(512, 1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         hidden_state = distilbert_output[0][:, 0]  # Extract the [CLS] token's hidden state\n",
        "#         attention_output = self.attention(hidden_state.unsqueeze(1)).squeeze(1)\n",
        "#         regression_output = self.regressor(attention_output)\n",
        "#         residual_output = self.residual(hidden_state)\n",
        "\n",
        "#         # Combine outputs from the main regressor and the residual path\n",
        "#         combined_output = regression_output + residual_output\n",
        "#         return combined_output"
      ],
      "metadata": {
        "id": "WXqLoXE8uiRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model Definition\n",
        "# class DistilBertForMultiLabelClassification(torch.nn.Module):\n",
        "#     def __init__(self, num_labels=120, dropout_rate=0.3):\n",
        "#         super().__init__()\n",
        "#         self.num_labels = num_labels\n",
        "#         self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", token=access_token)\n",
        "#         self.config = self.distilbert.config\n",
        "\n",
        "#         # Expanded complex structure\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "#         self.dense1 = nn.Linear(self.distilbert.config.dim, self.distilbert.config.dim * 2)\n",
        "#         self.activation1 = nn.GELU()\n",
        "#         self.norm1 = nn.LayerNorm(self.distilbert.config.dim * 2)\n",
        "#         self.dense2 = nn.Linear(self.distilbert.config.dim * 2, self.distilbert.config.dim * 2)\n",
        "#         self.activation2 = nn.GELU()\n",
        "#         self.norm2 = nn.LayerNorm(self.distilbert.config.dim * 2)\n",
        "#         self.dense3 = nn.Linear(self.distilbert.config.dim * 2, self.distilbert.config.dim)  # bottleneck layer\n",
        "#         self.activation3 = nn.GELU()\n",
        "#         self.norm3 = nn.LayerNorm(self.distilbert.config.dim)\n",
        "\n",
        "#         # Adding residual connections\n",
        "#         self.residual1 = nn.Linear(self.distilbert.config.dim, self.distilbert.config.dim * 2)\n",
        "#         self.residual2 = nn.Linear(self.distilbert.config.dim * 2, self.distilbert.config.dim * 2)\n",
        "\n",
        "#         # Final classifier layer\n",
        "#         self.classifier = nn.Linear(self.distilbert.config.dim, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         hidden_state = distilbert_output[0]  # (bs, seq_length, dim)\n",
        "#         pooled_output = hidden_state[:, 0]  # use the first token, typically [CLS]\n",
        "\n",
        "#         # Passing through the complex network layers with residual connections\n",
        "#         x = self.dropout(pooled_output)\n",
        "#         x = self.dense1(x) + self.residual1(pooled_output)\n",
        "#         x = self.activation1(x)\n",
        "#         x = self.norm1(x)\n",
        "\n",
        "#         x = self.dense2(x) + self.residual2(x)\n",
        "#         x = self.activation2(x)\n",
        "#         x = self.norm2(x)\n",
        "\n",
        "#         x = self.dense3(x)\n",
        "#         x = self.activation3(x)\n",
        "#         x = self.norm3(x)\n",
        "\n",
        "#         # Classifier\n",
        "#         logits = self.classifier(x)\n",
        "#         return logits"
      ],
      "metadata": {
        "id": "GHHVZmh1wYu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBertSequenceModel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4, token=access_token)"
      ],
      "metadata": {
        "id": "nx7_h8EraK_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Models**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rBn2z7dku3Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_model_scaler(model_path, scaler_path, model, device):\n",
        "#     model_state_dict = torch.load(os.path.join(model_path, 'model_state.bin'), map_location=device)\n",
        "#     model.load_state_dict(model_state_dict)\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     if \"mlb\" in scaler_path:\n",
        "#         with open(os.path.join(scaler_path, 'mlb.pkl'), 'rb') as mlb_file:\n",
        "#             scaler = pickle.load(mlb_file)\n",
        "#     elif \"label_encoder\" in scaler_path:\n",
        "#         with open(os.path.join(scaler_path, 'le.pkl'), 'rb') as label_encoder_file:\n",
        "#             scaler = pickle.load(label_encoder_file)\n",
        "#     else:\n",
        "#       with open(os.path.join(scaler_path, 'scaler.pkl'), 'rb') as scaler_file:\n",
        "#           scaler = pickle.load(scaler_file)\n",
        "\n",
        "#     return model, scaler\n",
        "\n",
        "# # Load model and scaler\n",
        "# cook_time_model, cook_time_scaler = load_model_scaler(cook_time_model_path, cook_time_scaler_path, DistilBertRegressor(), device)\n",
        "# tags_model, tags_mlb = load_model_scaler(tags_model_path, tags_mlb_path, DistilBertForMultiLabelClassification(), device)\n",
        "# segmentation_model, segmentation_le = load_model_scaler(segmentation_model_path, segmentation_le_path, DistilBertSequenceModel, device)"
      ],
      "metadata": {
        "id": "ZwJI8XPuQuPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_scaler(scaler_path):\n",
        "    with open(scaler_path, \"rb\") as scaler_file:\n",
        "        scaler = pickle.load(scaler_file)\n",
        "    return scaler\n",
        "\n",
        "# Load model and scaler\n",
        "cook_time_scaler = load_scaler(cook_time_scaler_path)\n",
        "tags_mlb = load_scaler(tags_mlb_path)\n",
        "segmentation_le = load_scaler(segmentation_le_path)"
      ],
      "metadata": {
        "id": "Ei2sYyScumZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define dummy inputs\n",
        "# def get_dummy_inputs(max_length=512):\n",
        "#     input_ids = torch.randint(0, 100, (1, max_length)).to(device)\n",
        "#     attention_mask = torch.ones_like(input_ids).to(device)\n",
        "#     return input_ids, attention_mask\n",
        "\n",
        "\n",
        "# # Convert model function\n",
        "# def convert_to_onnx(model, model_name, dummy_inputs, output_path):\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     input_names = [\"input_ids\", \"attention_mask\"]\n",
        "#     output_names = [\"output\"]\n",
        "#     dynamic_axes = {\n",
        "#         \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "#         \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "#         \"output\": {0: \"batch_size\"}\n",
        "#     }\n",
        "\n",
        "#     torch.onnx.export(\n",
        "#         model,\n",
        "#         dummy_inputs,\n",
        "#         output_path,\n",
        "#         input_names=input_names,\n",
        "#         output_names=output_names,\n",
        "#         dynamic_axes=dynamic_axes,\n",
        "#         opset_version=11\n",
        "#     )\n",
        "#     print(f\"{model_name} converted to ONNX and saved at {output_path}\")\n",
        "\n",
        "# # Load and convert DistilBertRegressor\n",
        "# dummy_inputs = get_dummy_inputs()\n",
        "# convert_to_onnx(cook_time_model, \"DistilBertRegressor\", dummy_inputs, cook_time_model_path_onnx)\n",
        "\n",
        "# # Load and convert DistilBertForMultiLabelClassification\n",
        "# dummy_inputs = get_dummy_inputs()\n",
        "# convert_to_onnx(tags_model, \"DistilBertForMultiLabelClassification\", dummy_inputs, tags_model_path_onnx)\n",
        "\n",
        "# # Load and convert DistilBertSequenceModel\n",
        "# convert_to_onnx(segmentation_model, \"DistilBertSequenceModel\", dummy_inputs, segmentation_model_path_onnx)"
      ],
      "metadata": {
        "id": "BZ3OC3JFyiCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ONNX model\n",
        "onnx_cook_time_session = ort.InferenceSession(cook_time_model_path_onnx)\n",
        "onnx_tags_session = ort.InferenceSession(tags_model_path_onnx)\n",
        "onnx_segmentation_session = ort.InferenceSession(segmentation_model_path_onnx)"
      ],
      "metadata": {
        "id": "YvCgxpcOIM7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Predictions**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jRki6zrh8fWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_texts(data, tokenizer):\n",
        "    if not isinstance(data, list):\n",
        "        data = [data]\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        data,\n",
        "        max_length=512,\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encoding['input_ids'], encoding['attention_mask']"
      ],
      "metadata": {
        "id": "u-N4FQNTeuX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input_text(data, tokenizer, model_name):\n",
        "    if model_name == \"cook_time\":\n",
        "        combined_text = data[\"name\"] + \" [SEP] \" + \" \".join(data[\"ingredients_units\"]) + \" [SEP] \" + \" \".join(data[\"steps\"]) + \" [SEP] \" + \" \".join(data[\"tags\"])\n",
        "    elif model_name == \"tags\":\n",
        "        combined_text = data[\"name\"] + \" [SEP] \" + \" \".join(data[\"ingredients_units\"]) + \" [SEP] \" + \" \".join(data[\"steps\"])\n",
        "\n",
        "    return encode_texts(combined_text, tokenizer) if model_name != \"segmentation\" else encode_texts(data, tokenizer)"
      ],
      "metadata": {
        "id": "6iYvGRYm8dn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onnx_predict(session, input_ids, attention_mask):\n",
        "    input_feed = {\n",
        "        \"input_ids\": input_ids.cpu().numpy(),\n",
        "        \"attention_mask\": attention_mask.cpu().numpy(),\n",
        "    }\n",
        "    outputs = session.run(None, input_feed)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "uJBQVEXxMnvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_regression_and_classification(model, model_name, input_ids, attention_mask, device, scaler):\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    if model_name == \"cook_time\":\n",
        "        outputs = onnx_predict(onnx_cook_time_session, input_ids, attention_mask)\n",
        "        predictions = outputs[0]\n",
        "    elif model_name == \"segmentation\":\n",
        "        outputs = onnx_predict(onnx_segmentation_session, input_ids, attention_mask)\n",
        "        logits = torch.tensor(outputs[0])\n",
        "        probs = softmax(logits, dim=1)\n",
        "        predicted_class = probs.argmax(dim=1)\n",
        "        predictions = predicted_class.numpy()\n",
        "    else:\n",
        "        outputs = onnx_predict(onnx_tags_session, input_ids, attention_mask)\n",
        "        probs = torch.tensor(outputs[0])\n",
        "        predictions = torch.sigmoid(probs)\n",
        "        predictions = predictions > 0.42\n",
        "        predictions = predictions.numpy()\n",
        "\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "KYl9yWIbMpLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nutrition API**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yPpkmvJMVKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nutrition(data):\n",
        "    APP_ID = \"a8208be0\"\n",
        "    API_KEY = \"71cfd0401d28ac067a1d8e1ad32ddf38\"\n",
        "    url = \"https://api.edamam.com/api/nutrition-details\"\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    recipe = {\n",
        "        \"title\": data[\"name\"],\n",
        "        \"ingr\": data[\"ingredients_units\"],\n",
        "    }\n",
        "\n",
        "    # Parameters including your credentials\n",
        "    params = {\"app_id\": APP_ID, \"app_key\": API_KEY}\n",
        "    response = requests.post(url, headers=headers, json=recipe, params=params)\n",
        "\n",
        "    response_data = None\n",
        "    total_weight = 0\n",
        "    health_labels = []\n",
        "    diet_labels = []\n",
        "    nutrition = {\n",
        "        \"calories\": \"\",\n",
        "        \"total_fat\": \"\",\n",
        "        \"saturated_fat\": \"\",\n",
        "        \"trans_fat\": \"\",\n",
        "        \"carbohydrates\": \"\",\n",
        "        \"fiber\": \"\",\n",
        "        \"sugars\": \"\",\n",
        "        \"protein\": \"\",\n",
        "        \"cholesterol\": \"\",\n",
        "        \"sodium\": \"\",\n",
        "    }\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_data = response.json()\n",
        "        nutrients = response_data['totalNutrients']\n",
        "        nutrition['calories'] = f\"{nutrients['ENERC_KCAL']['quantity']:.2f} {nutrients['ENERC_KCAL']['unit']}\"\n",
        "        nutrition['total_fat'] = f\"{nutrients['FAT']['quantity']:.2f} {nutrients['FAT']['unit']}\"\n",
        "        nutrition['saturated_fat'] = f\"{nutrients['FASAT']['quantity']:.2f} {nutrients['FASAT']['unit']}\"\n",
        "        nutrition['trans_fat'] = f\"{nutrients['FATRN']['quantity']:.2f} {nutrients['FATRN']['unit']}\"\n",
        "        nutrition['carbohydrates'] = f\"{nutrients['CHOCDF']['quantity']:.2f} {nutrients['CHOCDF']['unit']}\"\n",
        "        nutrition['fiber'] = f\"{nutrients['FIBTG']['quantity']:.2f} {nutrients['FIBTG']['unit']}\"\n",
        "        nutrition['sugars'] = f\"{nutrients['SUGAR']['quantity']:.2f} {nutrients['SUGAR']['unit']}\"\n",
        "        nutrition['cholesterol'] = f\"{nutrients['CHOLE']['quantity']:.2f} {nutrients['CHOLE']['unit']}\"\n",
        "        nutrition['sodium'] = f\"{nutrients['NA']['quantity']:.2f} {nutrients['NA']['unit']}\"\n",
        "        nutrition['protein'] = f\"{nutrients['PROCNT']['quantity']:.2f} {nutrients['PROCNT']['unit']}\"\n",
        "\n",
        "        health_labels = response_data['healthLabels']\n",
        "        diet_labels = response_data['dietLabels']\n",
        "        total_weight = response_data['totalWeight']\n",
        "\n",
        "        health_labels = [health_label.lower() for health_label in health_labels]\n",
        "        diet_labels = [diet_label.lower() for diet_label in diet_labels]\n",
        "    else:\n",
        "        print(\"Failed to fetch data:\", response.status_code)\n",
        "\n",
        "\n",
        "    return nutrition, health_labels, diet_labels, total_weight"
      ],
      "metadata": {
        "id": "Ym_znG0TAN13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Whole Model**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OrLo4tFEVdX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if isinstance(text, list):\n",
        "        return [clean_text_single(t) for t in text]\n",
        "    else:\n",
        "        return clean_text_single(text)\n",
        "\n",
        "def clean_text_single(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'#\\S+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'^\\d+\\.\\s+', '', text) # Remove numbers followed by a dot at the start of text\n",
        "    text = re.sub(r'(\\d+)(?=[^\\d\\s/\\-.])', r'\\1 ', text)  # Add space after numbers, unless followed by punctuation\n",
        "    text = re.sub(r'[^\\w\\s/-]', '', text)  # Remove punctuation but keep /, -\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove emojis\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters like emojis\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace and strip leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "def clean_list(strings):\n",
        "    return [' '.join(s.split()) for s in strings if s.strip()]"
      ],
      "metadata": {
        "id": "D7iKkgOga5S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"CRISPY CAULIFLOWER AND GREEN TAHINI\n",
        "\n",
        "Perfect for entertaining or as a healthy side dish\n",
        "\n",
        "Ingredients:\n",
        "Cauliflower\n",
        "Olive oil\n",
        "Dried garlic\n",
        "Oregano\n",
        "Onion powder\n",
        "S&P\n",
        "\n",
        "Green Tahini sauce:\n",
        "1/2 cup Hulled tahini\n",
        "1 bunch Parsley\n",
        "1 bunch Mint\n",
        "3 Garlic cloves\n",
        "2 Lemons juiced\n",
        "2 tablespoons honey\n",
        "\n",
        "Method:\n",
        "Heat oven to 200 degrees\n",
        "Chop cauliflower into florets and roast with all non sauce ingredients for 45 mins\n",
        "Use food processor to combine tahini sauce ingredients\n",
        "Once cauliflower is crispy, take out of the oven and spoon tahini sauce on top\n",
        "Serve as a side to any meal\n",
        "\n",
        "This is my new dinner.\n",
        "\"\"\"\n",
        "\n",
        "lines = text.splitlines()\n",
        "texts = [line for line in lines if line]\n",
        "cleaned_texts = clean_text(texts)"
      ],
      "metadata": {
        "id": "ArCYMLV6am09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "COOK ONCE EAT TWICE\n",
        "\n",
        "~ CHICKEN KORMA TRAY-BAKE ~\n",
        "\n",
        "Looking a quick and easy, warming, mid-week dinner? This oneâ€™s for you.\n",
        "\n",
        "A great option for the whole family and perfect for leftovers.\n",
        "\n",
        "The chicken can be marinated ahead of time and even frozen, ready for a later date.\n",
        "\n",
        "Ingredients:\n",
        "\n",
        "Serves 5\n",
        "\n",
        "500g chicken thighs\n",
        "2 tbs natural yoghurt\n",
        "1 tbs curry paste (bought or homemade)\n",
        "\n",
        "Curry Paste:\n",
        "3 tbs cashew nuts\n",
        "1 whole star anise\n",
        "2 1/2 tsp cumin seeds\n",
        "1 1/2 tbs coriander seeds\n",
        "1/2 tsp turmeric\n",
        "1 tsp garam masala\n",
        "1/2 tsp chilli powder\n",
        "2 tbs olive oil\n",
        "3 cloves garlic\n",
        "2 tbs grated ginger\n",
        "2 tbs tomato paste\n",
        "1 1/2 tbs dessicated coconut\n",
        "3 tbs fresh coriander\n",
        "\n",
        "1 onion, diced\n",
        "1 tbs grated ginger\n",
        "2 cloves garlic, crushed\n",
        "1/2 cup of Korma curry paste\n",
        "1/2 tsp ground cardamom\n",
        "2 tbs tomato paste\n",
        "400mL coconut milk\n",
        "1 bunch Dutch carrots\n",
        "1 medium head of broccoli, cut into florets\n",
        "1-2 cups chopped spinach\n",
        "Rice, to serve\n",
        "\n",
        "Method:\n",
        "1. Put chicken, yoghurt and 1 tbs of curry paste into a bowl. Mix to combine. Set aside for at least 20 mins to marinate.\n",
        "2. Preheat the oven to 180C.\n",
        "3. Heat an oven-proof tray/pan over medium heat and add a drizzle of olive oil and chicken. Cook for 1-2 mins each side to brown. Remove from pan.\n",
        "4. Add onion, garlic and ginger and sautÃ© for 3-4 minutes. Add curry paste, cardamom, tomato paste and coconut milk. Mix to combine and bring to a simmer.\n",
        "5. Add chicken back in and carrots and put tray into the oven and cook for 20 minutes.\n",
        "6. Chop broccoli and spinach. After 20 minutes, mix in the spinach and add the broccoli. Cook for a further 10 minutes.\n",
        "7. Remove from the oven, add coriander and serve with rice or naan bread.\n",
        "\n",
        "#korma #kormacurry #chickenkorma #indiancooking #dinner #weeknightdinner #quickdinner #onetraydinner #onepanmeal #mealideas #dinnerideas #quickandeasydinner #midweekmeal #traybake #cookonceeattwice #curry #winterdinners #whatspruecooking #dietitian #nutrition #nutritionist #wholefoods #recipeideas #mumof3 #family #familymealideas #homecook #wholesomefamilymeals #dietitianapproved #dietitiansofinstagram\n",
        "\"\"\"\n",
        "\n",
        "lines = text.splitlines()\n",
        "texts = [line for line in lines if line]\n",
        "cleaned_texts = clean_list(clean_text(texts))\n"
      ],
      "metadata": {
        "id": "Q0eTfm0aG96A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "~ ANZAC BISCUITS ~\n",
        "\n",
        "Tomorrow is ANZAC Day.\n",
        "\n",
        "Each year around Anzac Day Iâ€™ll make a batch or two of Anzac biscuits and think â€˜why donâ€™t I make these all year roundâ€™ ðŸ¤”.\n",
        "\n",
        "Theyâ€™re so delicious and they keep for ages in an airtight container, which is how they were designed back in the day to be able to be shipped to the soldiers fighting to defend our country.\n",
        "\n",
        "Lest we forget.\n",
        "\n",
        "Ingredients:\n",
        "1 1/2 cups rolled oats\n",
        "1 cup plain flour\n",
        "1/2 cup raw sugar\n",
        "3/4 cup shredded coconut\n",
        "125g butter\n",
        "1/3 cup golden syrup\n",
        "1 tsp bicarb soda\n",
        "2 tbs boiling water\n",
        "\n",
        "Method:\n",
        "1. Preheat the oven to 16oÂºC and line a baking tray with baking paper.\n",
        "2. Combine oats, flour, sugar and coconut h into a bowl and mix to combine.\n",
        "3. Place the butter and golden syrup into and small saucepan and heat over medium heat until the mixture starts to bubble slightly.\n",
        "4. Place the bicarb soda and water into a small bowl, mix quickly then pour into the saucepan with the golden syrup. 5. Mix and quickly pour into the bowl with the dry ingredients and mix well to combine.\n",
        "6. Roll spoonfuls of the mix into balls and place onto the tray. Gently press down with a fork and repeat with the remaining mixture.\n",
        "7. Bake for ~12-15 minutes or until golden brown.\n",
        "\n",
        "#anzacday #anzacbiscuits #anzacbiscuitrecipe #lestweforget #whatspruecooking #dietitian #nutrition #nutritionist #delicious #kidssnacks #snacks #lunchboxsnacks #family #familysnacks #wholesomefamilymeals #wholesomesnacks #deliciousbiscuits #atthegoingdownofthesun #anzac #anzacspirit\n",
        "\"\"\"\n",
        "\n",
        "lines = text.splitlines()\n",
        "texts = [line for line in lines if line]\n",
        "cleaned_texts = clean_list(clean_text(texts))"
      ],
      "metadata": {
        "id": "TMTdVOsOWnlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp-fQNfmmNcJ",
        "outputId": "bd76cd7d-4233-4ca1-ab95-98e41f97a2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crispy cauliflower and green tahini',\n",
              " 'perfect for entertaining or as a healthy side dish',\n",
              " 'ingredients',\n",
              " 'cauliflower',\n",
              " 'olive oil',\n",
              " 'dried garlic',\n",
              " 'oregano',\n",
              " 'onion powder',\n",
              " 'sp',\n",
              " 'green tahini sauce',\n",
              " '1/2 cup hulled tahini',\n",
              " '1 bunch parsley',\n",
              " '1 bunch mint',\n",
              " '3 garlic cloves',\n",
              " '2 lemons juiced',\n",
              " '2 tablespoons honey',\n",
              " 'method',\n",
              " 'heat oven to 200 degrees',\n",
              " 'chop cauliflower into florets and roast with all non sauce ingredients for 45 mins',\n",
              " 'use food processor to combine tahini sauce ingredients',\n",
              " 'once cauliflower is crispy take out of the oven and spoon tahini sauce on top',\n",
              " 'serve as a side to any meal',\n",
              " 'this is my new dinner']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "ingredients_model = spacy.load(\"/content/drive/MyDrive/ingredient_parsing_model\")"
      ],
      "metadata": {
        "id": "xEiHO7uMFLSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_model(texts, model_name, model, scaler):\n",
        "    \"\"\" Predicts tags for given recipe data. \"\"\"\n",
        "    input_ids, attention_mask = preprocess_input_text(texts, tokenizer, model_name)\n",
        "    predictions = predict_regression_and_classification(model, model_name, input_ids, attention_mask, device, scaler)\n",
        "\n",
        "    if model_name == \"tags\":\n",
        "        predictions = list(predictions[0])\n",
        "    elif model_name == \"segmentation\":\n",
        "        predictions = [(text, label) for text, label in zip(texts, predictions)]\n",
        "    else:\n",
        "        predictions = 5 * round(predictions[0][0] / 5)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "n6CthiCgFMwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_ingredients(text, model):\n",
        "    \"\"\" Parses ingredients from text using a Spacy NER model. \"\"\"\n",
        "    doc = model(text)\n",
        "    return [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n",
        "\n",
        "def predict_ingredients(raw_ingredients, ingredients_model):\n",
        "    \"\"\" Predicts ingredients details and extracts unique ingredients set. \"\"\"\n",
        "    ingredient_details, NER_ingredients = {}, set()\n",
        "\n",
        "    for name, ingredients_list in raw_ingredients.items():\n",
        "        ingredient_details[name] = {}\n",
        "        for ingredient_text in ingredients_list:\n",
        "            parsed_ingredients = parse_ingredients(ingredient_text, ingredients_model)\n",
        "            food_name, serving_size, unit, modifier, colour, physical_quality = 'Unknown', 'unspecified', '', '', '', ''\n",
        "\n",
        "            for item in parsed_ingredients:\n",
        "                if item['label'] == 'QUANTITY':\n",
        "                    serving_size = item['text']\n",
        "                elif item['label'] == 'UNIT':\n",
        "                    unit = item['text']\n",
        "                elif item['label'] in ['PROCESS']:\n",
        "                    modifier = item['text']\n",
        "                elif item['label'] == 'COLOR':\n",
        "                    colour = item['text']\n",
        "                elif item['label'] == 'PHYSICAL_QUALITY':\n",
        "                    physical_quality = item['text']\n",
        "                elif item['label'] == 'FOOD':\n",
        "                    food_name = item['text']\n",
        "\n",
        "            food_name = \" \".join(f\"{colour} {physical_quality} {food_name}\".split()) if food_name != 'Unknown' else ingredient_text\n",
        "            serving = \" \".join(f\"{serving_size} {unit} {modifier}\".split())\n",
        "            NER_ingredients.add(food_name)\n",
        "            ingredient_details[name][food_name] = serving\n",
        "\n",
        "    return ingredient_details, NER_ingredients\n",
        "\n",
        "def categorize_text(data):\n",
        "    \"\"\" Categorizes text into recipes or steps based on their labels. \"\"\"\n",
        "    recipes, steps = {}, []\n",
        "    current_name, first_name = None, None\n",
        "\n",
        "    for text, label in data:\n",
        "        if text in ['method', 'steps', 'ingredients', 'ingredient', 'method:']:\n",
        "            continue\n",
        "\n",
        "        if label == 'name':\n",
        "            current_name = text\n",
        "            recipes[current_name] = []\n",
        "            if not first_name:\n",
        "                first_name = text\n",
        "        elif label == 'ingredients' and current_name:\n",
        "            recipes[current_name].append(text)\n",
        "            if first_name and first_name != current_name:\n",
        "                recipes[first_name].append(text)\n",
        "        elif label == 'steps':\n",
        "            steps.append(text)\n",
        "\n",
        "    return recipes, steps\n",
        "\n",
        "def format_steps(raw_steps):\n",
        "    \"\"\" Formats the raw steps into a numbered dictionary. \"\"\"\n",
        "    return {idx + 1: step for idx, step in enumerate(raw_steps)}\n",
        "\n",
        "def get_servings(full_servings, tags):\n",
        "    \"\"\" Determines number of servings and servings size based on tags. \"\"\"\n",
        "    servings_size = 200\n",
        "    servings_size_map = {\n",
        "        \"desserts\": 135,\n",
        "        \"cookies-and-brownies\": 135,\n",
        "        \"meat\": 400,\n",
        "        \"poultry\": 400,\n",
        "        \"vegetarian\": 300,\n",
        "        \"vegetables\": 300\n",
        "    }\n",
        "    for tag in tags:\n",
        "        if tag in servings_size_map:\n",
        "            servings_size = servings_size_map[tag]\n",
        "\n",
        "    servings = full_servings // servings_size\n",
        "    return int(servings), servings_size\n",
        "\n",
        "def get_difficulty(number_of_ingredients, number_of_steps):\n",
        "    \"\"\" Determines the cooking difficulty based on ingredients and steps. \"\"\"\n",
        "    total_items = number_of_ingredients + number_of_steps\n",
        "    if total_items <= 20:\n",
        "        return \"easy\"\n",
        "    elif total_items <= 40:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"hard\"\n",
        "\n",
        "# segmentation model\n",
        "segmented_data = predict_with_model(cleaned_texts, \"segmentation\", onnx_segmentation_session, segmentation_le)\n",
        "raw_ingredients, raw_steps = categorize_text(segmented_data)\n",
        "\n",
        "recipe_name = (next(iter(raw_ingredients)))\n",
        "sub_names = [name for name in raw_ingredients.keys() if name != recipe_name]\n",
        "\n",
        "nutrition_recipe_data = {\n",
        "    \"name\": recipe_name,\n",
        "    \"ingredients_units\": raw_ingredients[recipe_name],\n",
        "}\n",
        "\n",
        "tags_recipe_data = nutrition_recipe_data.copy()\n",
        "tags_recipe_data[\"steps\"] = raw_steps\n",
        "\n",
        "# tag model\n",
        "tags = predict_with_model(tags_recipe_data, \"tags\", onnx_tags_session, tags_mlb)\n",
        "\n",
        "cook_time_recipe_data = tags_recipe_data.copy()\n",
        "cook_time_recipe_data[\"tags\"] = tags\n",
        "\n",
        "# cook time model\n",
        "cook_time = predict_with_model(cook_time_recipe_data, \"cook_time\", onnx_cook_time_session, cook_time_scaler)\n",
        "\n",
        "# ingredient parsing model\n",
        "ingredients, NER_ingredients = predict_ingredients(raw_ingredients, ingredients_model)\n",
        "\n",
        "# nutrition api\n",
        "nutrition, health_labels, diet_labels, full_servings = get_nutrition(nutrition_recipe_data)\n",
        "\n",
        "servings, serving_size = get_servings(full_servings, tags)\n",
        "main_ingredients = ingredients[recipe_name]\n",
        "sub_ingredients = {name: ingredients[name] for name in sub_names}\n",
        "steps = format_steps(raw_steps)\n",
        "difficulty = get_difficulty(len(main_ingredients), len(steps))\n",
        "\n",
        "final_recipe = {\n",
        "    \"name\": recipe_name,\n",
        "    \"tags\": {\n",
        "        \"difficulty\": difficulty,\n",
        "        \"servings\": servings,\n",
        "        \"serving_size_grams\": serving_size,\n",
        "        \"cooking_time_minutes\": cook_time,\n",
        "        \"tags\": tags\n",
        "    },\n",
        "    \"ingredients\": main_ingredients,\n",
        "    \"steps\": steps,\n",
        "    \"nutrition\": nutrition,\n",
        "    \"health_labels\": health_labels,\n",
        "    \"diet_labels\": diet_labels,\n",
        "    \"NER\": list(NER_ingredients),\n",
        "    \"sub_recipes\": []\n",
        "}\n",
        "\n",
        "if sub_names:\n",
        "    for recipe_name in sub_names:\n",
        "        final_recipe[\"sub_recipes\"].append(\n",
        "            {\n",
        "                \"name\": recipe_name,\n",
        "                \"ingredients\": sub_ingredients[recipe_name]\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(json.dumps(final_recipe, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hngFZaWNWLoZ",
        "outputId": "c986d4c7-b480-4cff-a86b-b00c10e9de2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data: 555\n",
            "{\n",
            "    \"name\": \"crispy cauliflower and green tahini\",\n",
            "    \"tags\": {\n",
            "        \"difficulty\": \"easy\",\n",
            "        \"servings\": 0,\n",
            "        \"serving_size_grams\": 300,\n",
            "        \"cooking_time_minutes\": 50,\n",
            "        \"tags\": [\n",
            "            \"30-minutes-or-less\",\n",
            "            \"60-minutes-or-less\",\n",
            "            \"main-dish\",\n",
            "            \"vegetables\",\n",
            "            \"vegetarian\"\n",
            "        ]\n",
            "    },\n",
            "    \"ingredients\": {\n",
            "        \"cauliflower\": \"unspecified\",\n",
            "        \"olive oil\": \"unspecified\",\n",
            "        \"dried garlic\": \"unspecified\",\n",
            "        \"oregano\": \"unspecified\",\n",
            "        \"onion powder\": \"unspecified\",\n",
            "        \"hulled tahini\": \"1/2 cup\",\n",
            "        \"parsley\": \"1 bunch\",\n",
            "        \"mint\": \"1 bunch\",\n",
            "        \"garlic\": \"3 cloves\",\n",
            "        \"lemons\": \"2 juiced\",\n",
            "        \"honey\": \"2 tablespoons\"\n",
            "    },\n",
            "    \"steps\": {\n",
            "        \"1\": \"heat oven to 200 degrees\",\n",
            "        \"2\": \"chop cauliflower into florets and roast with all non sauce ingredients for 45 mins\",\n",
            "        \"3\": \"use food processor to combine tahini sauce ingredients\",\n",
            "        \"4\": \"once cauliflower is crispy take out of the oven and spoon tahini sauce on top\"\n",
            "    },\n",
            "    \"nutrition\": {\n",
            "        \"calories\": \"\",\n",
            "        \"total_fat\": \"\",\n",
            "        \"saturated_fat\": \"\",\n",
            "        \"trans_fat\": \"\",\n",
            "        \"carbohydrates\": \"\",\n",
            "        \"fiber\": \"\",\n",
            "        \"sugars\": \"\",\n",
            "        \"protein\": \"\",\n",
            "        \"cholesterol\": \"\",\n",
            "        \"sodium\": \"\"\n",
            "    },\n",
            "    \"health_labels\": [],\n",
            "    \"diet_labels\": [],\n",
            "    \"NER\": [\n",
            "        \"hulled tahini\",\n",
            "        \"parsley\",\n",
            "        \"onion powder\",\n",
            "        \"dried garlic\",\n",
            "        \"olive oil\",\n",
            "        \"honey\",\n",
            "        \"garlic\",\n",
            "        \"mint\",\n",
            "        \"oregano\",\n",
            "        \"lemons\",\n",
            "        \"cauliflower\"\n",
            "    ],\n",
            "    \"sub_recipes\": [\n",
            "        {\n",
            "            \"name\": \"green tahini sauce\",\n",
            "            \"ingredients\": {\n",
            "                \"hulled tahini\": \"1/2 cup\",\n",
            "                \"parsley\": \"1 bunch\",\n",
            "                \"mint\": \"1 bunch\",\n",
            "                \"garlic\": \"3 cloves\",\n",
            "                \"lemons\": \"2 juiced\",\n",
            "                \"honey\": \"2 tablespoons\"\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from flask import Flask, request, jsonify\n",
        "# import re\n",
        "\n",
        "# app = Flask(__name__)\n",
        "# model = spacy.load(\"./models/ingredient_parsing_model_v1\")\n",
        "\n",
        "\n",
        "\n",
        "# @app.route('/predict', methods=['POST'])\n",
        "# def predict():\n",
        "#     if not request.is_json:\n",
        "#         return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "#     data = request.get_json()\n",
        "#     if 'text' not in data:\n",
        "#         return jsonify({\"error\": \"Missing 'text' field in JSON data\"}), 400\n",
        "\n",
        "#     text = data['text']\n",
        "#     if not isinstance(text, str) or text.strip() == \"\":\n",
        "#         return jsonify({\"error\": \"The 'text' field must be a non-empty string\"}), 400\n",
        "\n",
        "#     text = re.sub(r'\\s+', ' ', text.strip())\n",
        "#     lines = text.splitlines()\n",
        "\n",
        "#     texts = [line for line in lines if line]\n",
        "#     cleaned_texts = clean_text(texts)\n",
        "\n",
        "#     segmented_data = predict_segmentation(cleaned_texts)\n",
        "#     raw_ingredients, raw_steps = categorize_text(segmented_data)\n",
        "\n",
        "#     recipe_name = (next(iter(raw_ingredients)))\n",
        "\n",
        "#     tags_recipe_data = {\n",
        "#         \"name\": recipe_name,\n",
        "#         \"ingredients_units\": raw_ingredients[recipe_name],\n",
        "#         \"steps\": raw_steps\n",
        "#     }\n",
        "\n",
        "#     tags = predict_tags(tags_recipe_data)\n",
        "#     ingredients = predict_ingredients(raw_ingredients)\n",
        "\n",
        "#     # Nutrition, Tagging, Cooking time\n",
        "#     def process_lines(lines):\n",
        "#         # Your model processing code would go here\n",
        "#         # This function should take the lines, apply model predictions and return results\n",
        "#         # Example:\n",
        "#         return {\"lines_processed\": len(lines), \"details\": processed_lines}\n",
        "\n",
        "#     # Get model predictions or processing results\n",
        "#     try:\n",
        "#         result = process_lines(lines)\n",
        "#     except Exception as e:\n",
        "#         return jsonify({\"error\": f\"Error processing the text: {str(e)}\"}), 500\n",
        "\n",
        "#     return jsonify(result)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(debug=True, host='0.0.0.0')\n"
      ],
      "metadata": {
        "id": "2Q7snriOvnv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tags to a binary matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "tags_encoded = mlb.fit_transform(df['tags'])\n",
        "tags_df = pd.DataFrame(tags_encoded, columns=mlb.classes_)\n",
        "\n",
        "# Create a feature matrix X and labels matrix Y\n",
        "X = np.random.rand(tags_encoded.shape[0], 10)  # Random feature data\n",
        "Y = tags_df.values\n",
        "\n",
        "# Initialize the MultiSmote object\n",
        "smote = MultiSmote()\n",
        "\n",
        "# Apply ML-SMOTE\n",
        "X_resampled, Y_resampled = smote.multi_smote(X, Y)\n",
        "\n",
        "# Convert the resampled labels back to DataFrame to check the new distribution\n",
        "Y_resampled_df = pd.DataFrame(Y_resampled, columns=mlb.classes_)\n",
        "new_tag_counts = Y_resampled_df.sum(axis=0)\n",
        "\n",
        "print(new_tag_counts)"
      ],
      "metadata": {
        "id": "UQBZMwSA86gw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}